{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26d04a8b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-02T11:23:34.197307Z",
     "iopub.status.busy": "2024-11-02T11:23:34.196127Z",
     "iopub.status.idle": "2024-11-02T11:23:35.178636Z",
     "shell.execute_reply": "2024-11-02T11:23:35.177379Z"
    },
    "papermill": {
     "duration": 0.996444,
     "end_time": "2024-11-02T11:23:35.181434",
     "exception": false,
     "start_time": "2024-11-02T11:23:34.184990",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/articles-data/echr_dataset/README\n",
      "/kaggle/input/articles-data/echr_dataset/Article3/ngrams_a3_procedure.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article3/ngrams_a3_relevantLaw.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article3/ngrams_a3_law.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article3/topics3.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article3/topics3_vocab.txt\n",
      "/kaggle/input/articles-data/echr_dataset/Article3/cases_a3.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article3/ngrams_a3_full.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article3/ngrams_a3_circumstances.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article3/ngrams_a3_featureNames.txt\n",
      "/kaggle/input/articles-data/echr_dataset/Article8/ngrams_a8_full.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article8/ngrams_a8_laws.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article8/ngrams_a8_procedure.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article8/topics8.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article8/ngrams_a8_relevantLaw.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article8/ngrams_a8_circumstanes.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article8/ngrams_a8_featureNames.txt\n",
      "/kaggle/input/articles-data/echr_dataset/Article8/cases_a8.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article8/topics8_vocab.txt\n",
      "/kaggle/input/articles-data/echr_dataset/Article6/ngrams_a6_circumstances.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article6/topics6.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article6/ngrams_a6_procedure.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article6/cases_a6.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article6/ngrams_a6_featureNames.txt\n",
      "/kaggle/input/articles-data/echr_dataset/Article6/ngrams_a6_relevantLaw.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article6/ngrams_a6_full.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article6/ngrams_a6_law.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article6/topics6_vocab.txt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "014cc105",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T11:23:35.196471Z",
     "iopub.status.busy": "2024-11-02T11:23:35.195923Z",
     "iopub.status.idle": "2024-11-02T11:23:35.210542Z",
     "shell.execute_reply": "2024-11-02T11:23:35.209472Z"
    },
    "papermill": {
     "duration": 0.024888,
     "end_time": "2024-11-02T11:23:35.213071",
     "exception": false,
     "start_time": "2024-11-02T11:23:35.188183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#For Article 3\n",
    "file_path = \"/kaggle/input/articles-data/echr_dataset/Article3/topics3_vocab.txt\"\n",
    "from collections import Counter \n",
    "\n",
    "# Step 2: Open the file and read its contents\n",
    "with open(file_path, 'r') as file:\n",
    "        # Read the file contents\n",
    "        text = file.read()\n",
    "        \n",
    "        # Remove commas and split the text into words\n",
    "        words = text.replace(',', ' ').split()\n",
    "        \n",
    "        # Step 3: Count the frequencies of each word\n",
    "        word_counts = Counter(words)\n",
    "        \n",
    "        # Step 4: Sort by frequency in descending order and get the top 2000 words\n",
    "        top_2000_words = word_counts.most_common(2000)\n",
    "    \n",
    "         # Step 5: Separate words and frequencies into two lists\n",
    "        words_array = [word for word, frequency in top_2000_words]\n",
    "        frequencies_array = [frequency for word, frequency in top_2000_words]\n",
    "        \n",
    "        # Print the top 2000 words and their frequencies\n",
    "        #for word, frequency in top_2000_words:\n",
    "            #print(f\"{word}: {frequency}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d43c92d",
   "metadata": {
    "papermill": {
     "duration": 0.005691,
     "end_time": "2024-11-02T11:23:35.225057",
     "exception": false,
     "start_time": "2024-11-02T11:23:35.219366",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Data Loading and Preprocessing Each article folder contains:\r\n",
    "\r\n",
    "cases_aN.csv for the labels (violation or no violation), N-gram feature files for different sections of each case, Topic representations and feature names. Start by loading the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47298680",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T11:23:35.239178Z",
     "iopub.status.busy": "2024-11-02T11:23:35.238744Z",
     "iopub.status.idle": "2024-11-02T11:23:37.299217Z",
     "shell.execute_reply": "2024-11-02T11:23:37.298045Z"
    },
    "papermill": {
     "duration": 2.070694,
     "end_time": "2024-11-02T11:23:37.302119",
     "exception": false,
     "start_time": "2024-11-02T11:23:35.231425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load labels\n",
    "labels_df = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article3/cases_a3.csv\")\n",
    "\n",
    "# Load N-gram features for each case section\n",
    "ngrams_full = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article3/ngrams_a3_full.csv\")\n",
    "ngrams_procedure = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article3/ngrams_a3_procedure.csv\")\n",
    "ngrams_circumstances = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article3/ngrams_a3_circumstances.csv\")\n",
    "ngrams_relevant_law = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article3/ngrams_a3_relevantLaw.csv\")\n",
    "ngrams_law = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article3/ngrams_a3_law.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5855da4d",
   "metadata": {
    "papermill": {
     "duration": 0.00607,
     "end_time": "2024-11-02T11:23:37.314434",
     "exception": false,
     "start_time": "2024-11-02T11:23:37.308364",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Feature Engineering\n",
    "Combine the features based on your case requirements. You might want to aggregate some sections, as specified:\n",
    "\n",
    "Facts = ngrams_aN_circumstances + ngrams_aN_relevantLaw+ procedure+ full + law , you can try out various possibilities . Follow the guidelines which decides the most cases hence the relevant feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3178e29b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T11:23:37.328913Z",
     "iopub.status.busy": "2024-11-02T11:23:37.328433Z",
     "iopub.status.idle": "2024-11-02T11:23:38.443619Z",
     "shell.execute_reply": "2024-11-02T11:23:38.442463Z"
    },
    "papermill": {
     "duration": 1.126216,
     "end_time": "2024-11-02T11:23:38.446731",
     "exception": false,
     "start_time": "2024-11-02T11:23:37.320515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combine relevant N-gram features (e.g., \"Facts\" = Circumstances + Relevant Law+ procedure + full +law)\n",
    "ngrams_facts = ngrams_circumstances.add(ngrams_relevant_law, fill_value=0)\n",
    "ngrams_facts = ngrams_facts.add(ngrams_procedure, fill_value=0)\n",
    "ngrams_facts = ngrams_facts.add(ngrams_law, fill_value=0)\n",
    "ngrams_facts = ngrams_facts.add(ngrams_full, fill_value=0)\n",
    "\n",
    "\n",
    "# Load topics and vocabulary files\n",
    "topics_df = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article3/topics3.csv\")\n",
    "ngrams_feature_names = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article3/ngrams_a3_featureNames.txt\")\n",
    "topics_vocab = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article3/ngrams_a3_featureNames.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd33f74f",
   "metadata": {
    "papermill": {
     "duration": 0.005943,
     "end_time": "2024-11-02T11:23:38.459077",
     "exception": false,
     "start_time": "2024-11-02T11:23:38.453134",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Topic Modeling\n",
    "The article suggests using spectral clustering on an N-gram similarity matrix to reduce dimensionality and create semantically meaningful topics.\n",
    "\n",
    "Step-by-Step for Topic Modeling:\n",
    "Compute N-gram Similarity: Use cosine similarity on the N-grams vector representation to create a similarity matrix.\n",
    "Apply Spectral Clustering: Perform clustering to group similar N-grams into 30 clusters, each representing a topic. We’ll use pre-trained clusters for simplicity, but you can adapt this to generate custom clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7fd3f6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T11:23:38.473153Z",
     "iopub.status.busy": "2024-11-02T11:23:38.472725Z",
     "iopub.status.idle": "2024-11-02T11:23:40.966641Z",
     "shell.execute_reply": "2024-11-02T11:23:40.965220Z"
    },
    "papermill": {
     "duration": 2.504604,
     "end_time": "2024-11-02T11:23:40.969838",
     "exception": false,
     "start_time": "2024-11-02T11:23:38.465234",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "# Create similarity matrix for N-grams\n",
    "ngram_vectors = ngrams_full.to_numpy()  # Assuming ngrams_full contains vector representations\n",
    "similarity_matrix = cosine_similarity(ngram_vectors)\n",
    "\n",
    "# Perform spectral clustering\n",
    "n_clusters = 30\n",
    "spectral_clust = SpectralClustering(n_clusters=n_clusters, affinity='precomputed', random_state=42)\n",
    "clusters = spectral_clust.fit_predict(similarity_matrix)\n",
    "\n",
    "# Add clusters as topics to topic features DataFrame\n",
    "topics_df['topic'] = clusters\n",
    "# Convert clusters (topics) into a DataFrame if it's not already\n",
    "topics_df = pd.DataFrame(clusters, columns=['topic'])\n",
    "\n",
    "# Concatenate topics to ngrams_facts along columns\n",
    "ngrams_facts = ngrams_facts.add(topics_df, fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a8fd08",
   "metadata": {
    "papermill": {
     "duration": 0.006016,
     "end_time": "2024-11-02T11:23:40.982337",
     "exception": false,
     "start_time": "2024-11-02T11:23:40.976321",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Prepare Labels\n",
    "Map labels to binary values: +1 for violation and -1 for no violation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e482042e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T11:23:40.997577Z",
     "iopub.status.busy": "2024-11-02T11:23:40.996938Z",
     "iopub.status.idle": "2024-11-02T11:23:41.605950Z",
     "shell.execute_reply": "2024-11-02T11:23:41.604396Z"
    },
    "papermill": {
     "duration": 0.619353,
     "end_time": "2024-11-02T11:23:41.607850",
     "exception": true,
     "start_time": "2024-11-02T11:23:40.988497",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original columns in labels_df: Index(['CASE OF SHISHKOV v. RUSSIA', 'v'], dtype='object')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'features_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m labels_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m labels_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mviolation_status\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnv\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Combine the dataframes as before\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m data_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([\u001b[43mfeatures_df\u001b[49m, labels_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#Feature labelling for X and y\u001b[39;00m\n\u001b[1;32m     18\u001b[0m X \u001b[38;5;241m=\u001b[39m data_df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Inspect the columns of labels_df\n",
    "print(\"Original columns in labels_df:\", labels_df.columns)\n",
    "\n",
    "# If there are more columns, select only the ones you need\n",
    "# Assuming the required columns are the first two:\n",
    "labels_df = labels_df.iloc[:, :2]  # Selecting the first two columns\n",
    "\n",
    "# Rename columns\n",
    "labels_df.columns = ['case_name', 'violation_status']\n",
    "\n",
    "# Map 'v' to +1 and 'nv' to -1 in the labels DataFrame\n",
    "labels_df['label'] = labels_df['violation_status'].apply(lambda x: -1 if x == 'nv' else 1)\n",
    "\n",
    "# Combine the dataframes as before\n",
    "data_df = pd.concat([features_df, labels_df['label']], axis=1).dropna()\n",
    "\n",
    "#Feature labelling for X and y\n",
    "X = data_df.drop('label', axis=1)\n",
    "y = data_df['label']\n",
    "\n",
    "# Verify shapes of X and y\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\")\n",
    "X = pd.get_dummies(X, drop_first=True) \n",
    "\n",
    "# Drop any rows with NaN values, if applicable\n",
    "X = X.dropna()\n",
    "\n",
    "\n",
    "# If y is a Pandas Series\n",
    "count = pd.Series(y).value_counts()\n",
    "print(\"Number of 1's:\", count.get(1))\n",
    "print(\"Number of -1's:\", count.get(-1))\n",
    "\n",
    "#Check whether the set is imbalanced or not\n",
    "\n",
    "# Get counts of all unique values\n",
    "counts = y.value_counts()\n",
    "print(counts)\n",
    "print(\"Balanced Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4eadb1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# 5. Train/Test Split and Model Training\n",
    "Perform a stratified 10-fold cross-validation using a linear SVM. Tune the regularization parameter $C$ with grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba27c45e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T11:10:24.647067Z",
     "iopub.status.busy": "2024-11-02T11:10:24.645895Z",
     "iopub.status.idle": "2024-11-02T11:11:06.693790Z",
     "shell.execute_reply": "2024-11-02T11:11:06.692214Z",
     "shell.execute_reply.started": "2024-11-02T11:10:24.646999Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "# Initialize SVM model\n",
    "svm_model = SVC()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Use StratifiedKFold with n_splits=5 to ensure class distribution is maintained\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Verify class distribution in each fold\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
    "    y_train_fold = y[train_idx]\n",
    "    unique_classes, counts = np.unique(y_train_fold, return_counts=True)\n",
    "    print(f\"Fold {fold_idx + 1} class distribution:\", dict(zip(unique_classes, counts)))\n",
    "\n",
    "# Initialize GridSearchCV with StratifiedKFold\n",
    "grid_search = GridSearchCV(svm_model, param_grid, cv=cv, scoring='accuracy', error_score='raise')\n",
    "\n",
    "# Fit the model with grid search\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Best model and score\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"Best Score: {grid_search.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8bcfd3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Similarly for Article 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764432f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T11:11:06.696606Z",
     "iopub.status.busy": "2024-11-02T11:11:06.696206Z",
     "iopub.status.idle": "2024-11-02T11:11:29.391506Z",
     "shell.execute_reply": "2024-11-02T11:11:29.390295Z",
     "shell.execute_reply.started": "2024-11-02T11:11:06.696567Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load labels\n",
    "labels_df = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article6/cases_a6.csv\")\n",
    "\n",
    "# Load N-gram features for each case section\n",
    "ngrams_full = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article6/ngrams_a6_full.csv\")\n",
    "ngrams_procedure = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article6/ngrams_a6_procedure.csv\")\n",
    "ngrams_circumstances = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article6/ngrams_a6_circumstances.csv\")\n",
    "ngrams_relevant_law = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article6/ngrams_a6_relevantLaw.csv\")\n",
    "ngrams_law = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article6/ngrams_a6_law.csv\")\n",
    "\n",
    "# Combine relevant N-gram features (e.g., \"Facts\" = Circumstances + Relevant Law+ procedure + full +law)\n",
    "ngrams_facts = ngrams_circumstances.add(ngrams_relevant_law, fill_value=0)\n",
    "ngrams_facts = ngrams_facts.add(ngrams_procedure, fill_value=0)\n",
    "ngrams_facts = ngrams_facts.add(ngrams_law, fill_value=0)\n",
    "ngrams_facts = ngrams_facts.add(ngrams_full, fill_value=0)\n",
    "# Load topics and vocabulary files\n",
    "topics_df = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article6/topics6.csv\")\n",
    "ngrams_feature_names = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article6/ngrams_a6_featureNames.txt\")\n",
    "topics_vocab = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article6/ngrams_a6_featureNames.txt\")\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "# Create similarity matrix for N-grams\n",
    "ngram_vectors = ngrams_full.to_numpy()  # Assuming ngrams_full contains vector representations\n",
    "similarity_matrix = cosine_similarity(ngram_vectors)\n",
    "\n",
    "# Perform spectral clustering\n",
    "n_clusters = 30\n",
    "spectral_clust = SpectralClustering(n_clusters=n_clusters, affinity='precomputed', random_state=42)\n",
    "clusters = spectral_clust.fit_predict(similarity_matrix)\n",
    "\n",
    "# Add clusters as topics to topic features DataFrame\n",
    "topics_df['topic'] = clusters\n",
    "# Convert clusters (topics) into a DataFrame if it's not already\n",
    "topics_df = pd.DataFrame(clusters, columns=['topic'])\n",
    "\n",
    "# Concatenate topics to ngrams_facts along columns\n",
    "ngrams_facts = pd.concat([ngrams_facts, topics_df], axis=1)\n",
    "# Inspect the columns of labels_df\n",
    "print(\"Original columns in labels_df:\", labels_df.columns)\n",
    "\n",
    "# If there are more columns, select only the ones you need\n",
    "# Assuming the required columns are the first two:\n",
    "labels_df = labels_df.iloc[:, :2]  # Selecting the first two columns\n",
    "\n",
    "# Rename columns\n",
    "labels_df.columns = ['case_name', 'violation_status']\n",
    "\n",
    "# Map 'v' to +1 and 'nv' to -1 in the labels DataFrame\n",
    "labels_df['label'] = labels_df['violation_status'].apply(lambda x: -1 if x == 'nv' else 1)\n",
    "\n",
    "# Combine the dataframes as before\n",
    "data_df = pd.concat([features_df, labels_df['label']], axis=1).dropna()\n",
    "\n",
    "#Feature labelling for X and y\n",
    "X = data_df.drop('label', axis=1)\n",
    "y = data_df['label']\n",
    "\n",
    "# Verify shapes of X and y\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\")\n",
    "X = pd.get_dummies(X, drop_first=True) \n",
    "\n",
    "# Drop any rows with NaN values, if applicable\n",
    "X = X.dropna()\n",
    "\n",
    "\n",
    "# If y is a Pandas Series\n",
    "count = pd.Series(y).value_counts()\n",
    "print(\"Number of 1's:\", count.get(1))\n",
    "print(\"Number of -1's:\", count.get(-1))\n",
    "\n",
    "# Get counts of all unique values\n",
    "counts = y.value_counts()\n",
    "print(counts)\n",
    "print(\"Balanced Set\")\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "# Initialize SVM model\n",
    "svm_model = SVC()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Use StratifiedKFold with n_splits=5 to ensure class distribution is maintained\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Verify class distribution in each fold\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
    "    y_train_fold = y[train_idx]\n",
    "    unique_classes, counts = np.unique(y_train_fold, return_counts=True)\n",
    "    print(f\"Fold {fold_idx + 1} class distribution:\", dict(zip(unique_classes, counts)))\n",
    "\n",
    "# Initialize GridSearchCV with StratifiedKFold\n",
    "grid_search = GridSearchCV(svm_model, param_grid, cv=cv, scoring='accuracy', error_score='raise')\n",
    "\n",
    "# Fit the model with grid search\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Best model and score\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"Best Score: {grid_search.best_score_}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6b6186",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Similarly for Article-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eae3f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T11:11:29.393546Z",
     "iopub.status.busy": "2024-11-02T11:11:29.393160Z",
     "iopub.status.idle": "2024-11-02T11:12:09.699094Z",
     "shell.execute_reply": "2024-11-02T11:12:09.697661Z",
     "shell.execute_reply.started": "2024-11-02T11:11:29.393505Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load labels\n",
    "labels_df = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article8/cases_a8.csv\")\n",
    "\n",
    "# Load N-gram features for each case section\n",
    "ngrams_full = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article8/ngrams_a8_full.csv\")\n",
    "ngrams_procedure = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article8/ngrams_a8_procedure.csv\")\n",
    "ngrams_circumstances = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article8/ngrams_a8_circumstanes.csv\")\n",
    "ngrams_relevant_law = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article8/ngrams_a8_relevantLaw.csv\")\n",
    "ngrams_law = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article8/ngrams_a8_laws.csv\")\n",
    "\n",
    "# Combine relevant N-gram features (e.g., \"Facts\" = Circumstances + Relevant Law+ procedure + full +law)\n",
    "ngrams_facts = ngrams_circumstances.add(ngrams_relevant_law, fill_value=0)\n",
    "ngrams_facts = ngrams_facts.add(ngrams_procedure, fill_value=0)\n",
    "ngrams_facts = ngrams_facts.add(ngrams_law, fill_value=0)\n",
    "ngrams_facts = ngrams_facts.add(ngrams_full, fill_value=0)\n",
    "# Load topics and vocabulary files\n",
    "topics_df = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article8/topics8.csv\")\n",
    "ngrams_feature_names = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article8/ngrams_a8_featureNames.txt\")\n",
    "topics_vocab = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article8/ngrams_a8_featureNames.txt\")\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "# Create similarity matrix for N-grams\n",
    "ngram_vectors = ngrams_full.to_numpy()  # Assuming ngrams_full contains vector representations\n",
    "similarity_matrix = cosine_similarity(ngram_vectors)\n",
    "\n",
    "# Perform spectral clustering\n",
    "n_clusters = 30\n",
    "spectral_clust = SpectralClustering(n_clusters=n_clusters, affinity='precomputed', random_state=42)\n",
    "clusters = spectral_clust.fit_predict(similarity_matrix)\n",
    "\n",
    "# Add clusters as topics to topic features DataFrame\n",
    "topics_df['topic'] = clusters\n",
    "# Convert clusters (topics) into a DataFrame if it's not already\n",
    "topics_df = pd.DataFrame(clusters, columns=['topic'])\n",
    "\n",
    "# Concatenate topics to ngrams_facts along columns\n",
    "ngrams_facts = pd.concat([ngrams_facts, topics_df], axis=1)\n",
    "# Inspect the columns of labels_df\n",
    "print(\"Original columns in labels_df:\", labels_df.columns)\n",
    "\n",
    "# If there are more columns, select only the ones you need\n",
    "# Assuming the required columns are the first two:\n",
    "labels_df = labels_df.iloc[:, :2]  # Selecting the first two columns\n",
    "\n",
    "# Rename columns\n",
    "labels_df.columns = ['case_name', 'violation_status']\n",
    "\n",
    "# Map 'v' to +1 and 'nv' to -1 in the labels DataFrame\n",
    "labels_df['label'] = labels_df['violation_status'].apply(lambda x: -1 if x == 'nv' else 1)\n",
    "\n",
    "# Combine the dataframes as before\n",
    "data_df = pd.concat([features_df, labels_df['label']], axis=1).dropna()\n",
    "\n",
    "#Feature labelling for X and y\n",
    "X = data_df.drop('label', axis=1)\n",
    "y = data_df['label']\n",
    "\n",
    "# Verify shapes of X and y\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\")\n",
    "X = pd.get_dummies(X, drop_first=True) \n",
    "\n",
    "# Drop any rows with NaN values, if applicable\n",
    "X = X.dropna()\n",
    "\n",
    "\n",
    "# If y is a Pandas Series\n",
    "count = pd.Series(y).value_counts()\n",
    "print(\"Number of 1's:\", count.get(1))\n",
    "print(\"Number of -1's:\", count.get(-1))\n",
    "\n",
    "# Get counts of all unique values\n",
    "counts = y.value_counts()\n",
    "print(counts)\n",
    "print(\"Balanced Set\")\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "# Initialize SVM model\n",
    "svm_model = SVC()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Use StratifiedKFold with n_splits=5 to ensure class distribution is maintained\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Verify class distribution in each fold\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
    "    y_train_fold = y[train_idx]\n",
    "    unique_classes, counts = np.unique(y_train_fold, return_counts=True)\n",
    "    print(f\"Fold {fold_idx + 1} class distribution:\", dict(zip(unique_classes, counts)))\n",
    "\n",
    "# Initialize GridSearchCV with StratifiedKFold\n",
    "grid_search = GridSearchCV(svm_model, param_grid, cv=cv, scoring='accuracy', error_score='raise')\n",
    "\n",
    "# Fit the model with grid search\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Best model and score\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"Best Score: {grid_search.best_score_}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5995606,
     "sourceId": 9785635,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11.255011,
   "end_time": "2024-11-02T11:23:42.335859",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-02T11:23:31.080848",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
