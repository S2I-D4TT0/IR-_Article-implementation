{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b439a4e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-02T13:34:49.781835Z",
     "iopub.status.busy": "2024-11-02T13:34:49.780491Z",
     "iopub.status.idle": "2024-11-02T13:34:50.773461Z",
     "shell.execute_reply": "2024-11-02T13:34:50.772075Z"
    },
    "papermill": {
     "duration": 1.004491,
     "end_time": "2024-11-02T13:34:50.776484",
     "exception": false,
     "start_time": "2024-11-02T13:34:49.771993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/articles-data/echr_dataset/README\n",
      "/kaggle/input/articles-data/echr_dataset/Article3/ngrams_a3_procedure.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article3/ngrams_a3_relevantLaw.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article3/ngrams_a3_law.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article3/topics3.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article3/topics3_vocab.txt\n",
      "/kaggle/input/articles-data/echr_dataset/Article3/cases_a3.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article3/ngrams_a3_full.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article3/ngrams_a3_circumstances.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article3/ngrams_a3_featureNames.txt\n",
      "/kaggle/input/articles-data/echr_dataset/Article8/ngrams_a8_full.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article8/ngrams_a8_laws.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article8/ngrams_a8_procedure.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article8/topics8.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article8/ngrams_a8_relevantLaw.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article8/ngrams_a8_circumstanes.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article8/ngrams_a8_featureNames.txt\n",
      "/kaggle/input/articles-data/echr_dataset/Article8/cases_a8.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article8/topics8_vocab.txt\n",
      "/kaggle/input/articles-data/echr_dataset/Article6/ngrams_a6_circumstances.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article6/topics6.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article6/ngrams_a6_procedure.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article6/cases_a6.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article6/ngrams_a6_featureNames.txt\n",
      "/kaggle/input/articles-data/echr_dataset/Article6/ngrams_a6_relevantLaw.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article6/ngrams_a6_full.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article6/ngrams_a6_law.csv\n",
      "/kaggle/input/articles-data/echr_dataset/Article6/topics6_vocab.txt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81c74d93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T13:34:50.791401Z",
     "iopub.status.busy": "2024-11-02T13:34:50.790808Z",
     "iopub.status.idle": "2024-11-02T13:34:50.805147Z",
     "shell.execute_reply": "2024-11-02T13:34:50.804016Z"
    },
    "papermill": {
     "duration": 0.024953,
     "end_time": "2024-11-02T13:34:50.808050",
     "exception": false,
     "start_time": "2024-11-02T13:34:50.783097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#For Article 3\n",
    "file_path = \"/kaggle/input/articles-data/echr_dataset/Article3/topics3_vocab.txt\"\n",
    "from collections import Counter \n",
    "\n",
    "# Step 2: Open the file and read its contents\n",
    "with open(file_path, 'r') as file:\n",
    "        # Read the file contents\n",
    "        text = file.read()\n",
    "        \n",
    "        # Remove commas and split the text into words\n",
    "        words = text.replace(',', ' ').split()\n",
    "        \n",
    "        # Step 3: Count the frequencies of each word\n",
    "        word_counts = Counter(words)\n",
    "        \n",
    "        # Step 4: Sort by frequency in descending order and get the top 2000 words\n",
    "        top_2000_words = word_counts.most_common(2000)\n",
    "    \n",
    "         # Step 5: Separate words and frequencies into two lists\n",
    "        words_array = [word for word, frequency in top_2000_words]\n",
    "        frequencies_array = [frequency for word, frequency in top_2000_words]\n",
    "        \n",
    "        # Print the top 2000 words and their frequencies\n",
    "        #for word, frequency in top_2000_words:\n",
    "            #print(f\"{word}: {frequency}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805922e0",
   "metadata": {
    "papermill": {
     "duration": 0.00601,
     "end_time": "2024-11-02T13:34:50.820842",
     "exception": false,
     "start_time": "2024-11-02T13:34:50.814832",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Data Loading and Preprocessing Each article folder contains:\r\n",
    "\r\n",
    "cases_aN.csv for the labels (violation or no violation), N-gram feature files for different sections of each case, Topic representations and feature names. Start by loading the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd3b55c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T13:34:50.835333Z",
     "iopub.status.busy": "2024-11-02T13:34:50.834823Z",
     "iopub.status.idle": "2024-11-02T13:34:53.119451Z",
     "shell.execute_reply": "2024-11-02T13:34:53.118062Z"
    },
    "papermill": {
     "duration": 2.295519,
     "end_time": "2024-11-02T13:34:53.122591",
     "exception": false,
     "start_time": "2024-11-02T13:34:50.827072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load labels\n",
    "labels_df = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article3/cases_a3.csv\")\n",
    "\n",
    "# Load N-gram features for each case section\n",
    "ngrams_full = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article3/ngrams_a3_full.csv\")\n",
    "ngrams_procedure = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article3/ngrams_a3_procedure.csv\")\n",
    "ngrams_circumstances = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article3/ngrams_a3_circumstances.csv\")\n",
    "ngrams_relevant_law = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article3/ngrams_a3_relevantLaw.csv\")\n",
    "ngrams_law = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article3/ngrams_a3_law.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fe67d7",
   "metadata": {
    "papermill": {
     "duration": 0.006115,
     "end_time": "2024-11-02T13:34:53.135463",
     "exception": false,
     "start_time": "2024-11-02T13:34:53.129348",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Feature Engineering\n",
    "Combine the features based on your case requirements. You might want to aggregate some sections, as specified:\n",
    "\n",
    "Facts = ngrams_aN_circumstances + ngrams_aN_relevantLaw+ procedure+ full + law , you can try out various possibilities . Follow the guidelines which decides the most cases hence the relevant feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "310d5fd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T13:34:53.150568Z",
     "iopub.status.busy": "2024-11-02T13:34:53.149428Z",
     "iopub.status.idle": "2024-11-02T13:34:54.277235Z",
     "shell.execute_reply": "2024-11-02T13:34:54.276031Z"
    },
    "papermill": {
     "duration": 1.138423,
     "end_time": "2024-11-02T13:34:54.280241",
     "exception": false,
     "start_time": "2024-11-02T13:34:53.141818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combine relevant N-gram features (e.g., \"Facts\" = Circumstances + Relevant Law+ procedure + full +law)\n",
    "ngrams_facts = ngrams_circumstances.add(ngrams_relevant_law, fill_value=0)\n",
    "ngrams_facts = ngrams_facts.add(ngrams_procedure, fill_value=0)\n",
    "ngrams_facts = ngrams_facts.add(ngrams_law, fill_value=0)\n",
    "ngrams_facts = ngrams_facts.add(ngrams_full, fill_value=0)\n",
    "\n",
    "\n",
    "# Load topics and vocabulary files\n",
    "topics_df = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article3/topics3.csv\")\n",
    "ngrams_feature_names = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article3/ngrams_a3_featureNames.txt\")\n",
    "topics_vocab = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article3/ngrams_a3_featureNames.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0093416",
   "metadata": {
    "papermill": {
     "duration": 0.006063,
     "end_time": "2024-11-02T13:34:54.292820",
     "exception": false,
     "start_time": "2024-11-02T13:34:54.286757",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Topic Modeling\n",
    "The article suggests using spectral clustering on an N-gram similarity matrix to reduce dimensionality and create semantically meaningful topics.\n",
    "\n",
    "Step-by-Step for Topic Modeling:\n",
    "Compute N-gram Similarity: Use cosine similarity on the N-grams vector representation to create a similarity matrix.\n",
    "Apply Spectral Clustering: Perform clustering to group similar N-grams into 30 clusters, each representing a topic. We’ll use pre-trained clusters for simplicity, but you can adapt this to generate custom clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16c14af0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T13:34:54.307540Z",
     "iopub.status.busy": "2024-11-02T13:34:54.307050Z",
     "iopub.status.idle": "2024-11-02T13:34:56.590723Z",
     "shell.execute_reply": "2024-11-02T13:34:56.589171Z"
    },
    "papermill": {
     "duration": 2.294517,
     "end_time": "2024-11-02T13:34:56.593792",
     "exception": false,
     "start_time": "2024-11-02T13:34:54.299275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "# Create similarity matrix for N-grams\n",
    "ngram_vectors = ngrams_full.to_numpy()  # Assuming ngrams_full contains vector representations\n",
    "similarity_matrix = cosine_similarity(ngram_vectors)\n",
    "\n",
    "# Perform spectral clustering\n",
    "n_clusters = 30\n",
    "spectral_clust = SpectralClustering(n_clusters=n_clusters, affinity='precomputed', random_state=42)\n",
    "clusters = spectral_clust.fit_predict(similarity_matrix)\n",
    "\n",
    "# Add clusters as topics to topic features DataFrame\n",
    "topics_df['topic'] = clusters\n",
    "# Convert clusters (topics) into a DataFrame if it's not already\n",
    "topics_df = pd.DataFrame(clusters, columns=['topic'])\n",
    "\n",
    "# Concatenate topics to ngrams_facts along columns\n",
    "ngrams_facts = pd.concat ([ngrams_facts,topics_df],axis=1).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a65afa",
   "metadata": {
    "papermill": {
     "duration": 0.005901,
     "end_time": "2024-11-02T13:34:56.606341",
     "exception": false,
     "start_time": "2024-11-02T13:34:56.600440",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Prepare Labels\n",
    "Map labels to binary values: +1 for violation and -1 for no violation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58095930",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T13:34:56.621837Z",
     "iopub.status.busy": "2024-11-02T13:34:56.621214Z",
     "iopub.status.idle": "2024-11-02T13:34:56.668367Z",
     "shell.execute_reply": "2024-11-02T13:34:56.667223Z"
    },
    "papermill": {
     "duration": 0.058849,
     "end_time": "2024-11-02T13:34:56.671380",
     "exception": false,
     "start_time": "2024-11-02T13:34:56.612531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original columns in labels_df: Index(['CASE OF SHISHKOV v. RUSSIA', 'v'], dtype='object')\n",
      "Shape of X: (249, 3450)\n",
      "Shape of y: (249,)\n",
      "Number of 1's: 124\n",
      "Number of -1's: 125\n",
      "label\n",
      "-1    125\n",
      " 1    124\n",
      "Name: count, dtype: int64\n",
      "Balanced Set\n"
     ]
    }
   ],
   "source": [
    "# Inspect the columns of labels_df\n",
    "print(\"Original columns in labels_df:\", labels_df.columns)\n",
    "\n",
    "# If there are more columns, select only the ones you need\n",
    "# Assuming the required columns are the first two:\n",
    "labels_df = labels_df.iloc[:, :2]  # Selecting the first two columns\n",
    "\n",
    "# Rename columns\n",
    "labels_df.columns = ['case_name', 'violation_status']\n",
    "\n",
    "# Map 'v' to +1 and 'nv' to -1 in the labels DataFrame\n",
    "labels_df['label'] = labels_df['violation_status'].apply(lambda x: -1 if x == 'nv' else 1)\n",
    "\n",
    "# Combine the dataframes as before\n",
    "data_df = pd.concat([ngrams_facts, labels_df['label']], axis=1).dropna()\n",
    "\n",
    "#Feature labelling for X and y\n",
    "X = data_df.drop('label', axis=1)\n",
    "y = data_df['label']\n",
    "\n",
    "# Verify shapes of X and y\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\")\n",
    "X = pd.get_dummies(X, drop_first=True) \n",
    "\n",
    "# Drop any rows with NaN values, if applicable\n",
    "X = X.dropna()\n",
    "\n",
    "\n",
    "# If y is a Pandas Series\n",
    "count = pd.Series(y).value_counts()\n",
    "print(\"Number of 1's:\", count.get(1))\n",
    "print(\"Number of -1's:\", count.get(-1))\n",
    "\n",
    "#Check whether the set is imbalanced or not\n",
    "\n",
    "# Get counts of all unique values\n",
    "counts = y.value_counts()\n",
    "print(counts)\n",
    "print(\"Balanced Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec2afa8",
   "metadata": {
    "papermill": {
     "duration": 0.005976,
     "end_time": "2024-11-02T13:34:56.683954",
     "exception": false,
     "start_time": "2024-11-02T13:34:56.677978",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Train/Test Split and Model Training\n",
    "Perform a stratified 10-fold cross-validation using a linear SVM. Tune the regularization parameter $C$ with grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe3ae655",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T13:34:56.698493Z",
     "iopub.status.busy": "2024-11-02T13:34:56.698012Z",
     "iopub.status.idle": "2024-11-02T13:35:10.778430Z",
     "shell.execute_reply": "2024-11-02T13:35:10.776937Z"
    },
    "papermill": {
     "duration": 14.091172,
     "end_time": "2024-11-02T13:35:10.781408",
     "exception": false,
     "start_time": "2024-11-02T13:34:56.690236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 class distribution: {-1: 100, 1: 99}\n",
      "Fold 2 class distribution: {-1: 100, 1: 99}\n",
      "Fold 3 class distribution: {-1: 100, 1: 99}\n",
      "Fold 4 class distribution: {-1: 100, 1: 99}\n",
      "Fold 5 class distribution: {-1: 100, 1: 100}\n",
      "Best Score: 0.730857142857143\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "# Initialize SVM model\n",
    "svm_model = SVC()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Use StratifiedKFold with n_splits=5 to ensure class distribution is maintained\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Verify class distribution in each fold\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
    "    y_train_fold = y[train_idx]\n",
    "    unique_classes, counts = np.unique(y_train_fold, return_counts=True)\n",
    "    print(f\"Fold {fold_idx + 1} class distribution:\", dict(zip(unique_classes, counts)))\n",
    "\n",
    "# Initialize GridSearchCV with StratifiedKFold\n",
    "grid_search = GridSearchCV(svm_model, param_grid, cv=cv, scoring='accuracy', error_score='raise')\n",
    "\n",
    "# Fit the model with grid search\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Best model and score\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"Best Score: {grid_search.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315e022f",
   "metadata": {
    "papermill": {
     "duration": 0.006302,
     "end_time": "2024-11-02T13:35:10.794564",
     "exception": false,
     "start_time": "2024-11-02T13:35:10.788262",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Similarly for Article 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d7cf1cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T13:35:10.810188Z",
     "iopub.status.busy": "2024-11-02T13:35:10.809751Z",
     "iopub.status.idle": "2024-11-02T13:35:19.657854Z",
     "shell.execute_reply": "2024-11-02T13:35:19.656243Z"
    },
    "papermill": {
     "duration": 8.859753,
     "end_time": "2024-11-02T13:35:19.660951",
     "exception": false,
     "start_time": "2024-11-02T13:35:10.801198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original columns in labels_df: Index(['CASE OF MIKHAYLOV v. RUSSIA', 'v'], dtype='object')\n",
      "Shape of X: (79, 2491)\n",
      "Shape of y: (79,)\n",
      "Number of 1's: 39\n",
      "Number of -1's: 40\n",
      "label\n",
      "-1    40\n",
      " 1    39\n",
      "Name: count, dtype: int64\n",
      "Balanced Set\n",
      "Fold 1 class distribution: {-1: 32, 1: 31}\n",
      "Fold 2 class distribution: {-1: 32, 1: 31}\n",
      "Fold 3 class distribution: {-1: 32, 1: 31}\n",
      "Fold 4 class distribution: {-1: 32, 1: 31}\n",
      "Fold 5 class distribution: {-1: 32, 1: 32}\n",
      "Best Score: 0.8591666666666666\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load labels\n",
    "labels_df = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article6/cases_a6.csv\")\n",
    "\n",
    "# Load N-gram features for each case section\n",
    "ngrams_full = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article6/ngrams_a6_full.csv\")\n",
    "ngrams_procedure = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article6/ngrams_a6_procedure.csv\")\n",
    "ngrams_circumstances = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article6/ngrams_a6_circumstances.csv\")\n",
    "ngrams_relevant_law = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article6/ngrams_a6_relevantLaw.csv\")\n",
    "ngrams_law = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article6/ngrams_a6_law.csv\")\n",
    "\n",
    "# Combine relevant N-gram features (e.g., \"Facts\" = Circumstances + Relevant Law+ procedure + full +law)\n",
    "ngrams_facts = ngrams_circumstances.add(ngrams_relevant_law, fill_value=0)\n",
    "ngrams_facts = ngrams_facts.add(ngrams_procedure, fill_value=0)\n",
    "ngrams_facts = ngrams_facts.add(ngrams_law, fill_value=0)\n",
    "ngrams_facts = ngrams_facts.add(ngrams_full, fill_value=0)\n",
    "# Load topics and vocabulary files\n",
    "topics_df = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article6/topics6.csv\")\n",
    "ngrams_feature_names = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article6/ngrams_a6_featureNames.txt\")\n",
    "topics_vocab = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article6/ngrams_a6_featureNames.txt\")\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "# Create similarity matrix for N-grams\n",
    "ngram_vectors = ngrams_full.to_numpy()  # Assuming ngrams_full contains vector representations\n",
    "similarity_matrix = cosine_similarity(ngram_vectors)\n",
    "\n",
    "# Perform spectral clustering\n",
    "n_clusters = 30\n",
    "spectral_clust = SpectralClustering(n_clusters=n_clusters, affinity='precomputed', random_state=42)\n",
    "clusters = spectral_clust.fit_predict(similarity_matrix)\n",
    "\n",
    "# Add clusters as topics to topic features DataFrame\n",
    "topics_df['topic'] = clusters\n",
    "# Convert clusters (topics) into a DataFrame if it's not already\n",
    "topics_df = pd.DataFrame(clusters, columns=['topic'])\n",
    "\n",
    "# Concatenate topics to ngrams_facts along columns\n",
    "ngrams_facts = pd.concat ([ngrams_facts,topics_df],axis=1).dropna()\n",
    "# Inspect the columns of labels_df\n",
    "print(\"Original columns in labels_df:\", labels_df.columns)\n",
    "\n",
    "# If there are more columns, select only the ones you need\n",
    "# Assuming the required columns are the first two:\n",
    "labels_df = labels_df.iloc[:, :2]  # Selecting the first two columns\n",
    "\n",
    "# Rename columns\n",
    "labels_df.columns = ['case_name', 'violation_status']\n",
    "\n",
    "# Map 'v' to +1 and 'nv' to -1 in the labels DataFrame\n",
    "labels_df['label'] = labels_df['violation_status'].apply(lambda x: -1 if x == 'nv' else 1)\n",
    "\n",
    "# Combine the dataframes as before\n",
    "data_df = pd.concat([ngrams_facts, labels_df['label']], axis=1).dropna()\n",
    "\n",
    "#Feature labelling for X and y\n",
    "X = data_df.drop('label', axis=1)\n",
    "y = data_df['label']\n",
    "\n",
    "# Verify shapes of X and y\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\")\n",
    "X = pd.get_dummies(X, drop_first=True) \n",
    "\n",
    "# Drop any rows with NaN values, if applicable\n",
    "X = X.dropna()\n",
    "\n",
    "\n",
    "# If y is a Pandas Series\n",
    "count = pd.Series(y).value_counts()\n",
    "print(\"Number of 1's:\", count.get(1))\n",
    "print(\"Number of -1's:\", count.get(-1))\n",
    "\n",
    "# Get counts of all unique values\n",
    "counts = y.value_counts()\n",
    "print(counts)\n",
    "print(\"Balanced Set\")\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "# Initialize SVM model\n",
    "svm_model = SVC()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Use StratifiedKFold with n_splits=5 to ensure class distribution is maintained\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Verify class distribution in each fold\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
    "    y_train_fold = y[train_idx]\n",
    "    unique_classes, counts = np.unique(y_train_fold, return_counts=True)\n",
    "    print(f\"Fold {fold_idx + 1} class distribution:\", dict(zip(unique_classes, counts)))\n",
    "\n",
    "# Initialize GridSearchCV with StratifiedKFold\n",
    "grid_search = GridSearchCV(svm_model, param_grid, cv=cv, scoring='accuracy', error_score='raise')\n",
    "\n",
    "# Fit the model with grid search\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Best model and score\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"Best Score: {grid_search.best_score_}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f28cb32",
   "metadata": {
    "papermill": {
     "duration": 0.006511,
     "end_time": "2024-11-02T13:35:19.674672",
     "exception": false,
     "start_time": "2024-11-02T13:35:19.668161",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Similarly for Article-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c3d40ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T13:35:19.690882Z",
     "iopub.status.busy": "2024-11-02T13:35:19.690400Z",
     "iopub.status.idle": "2024-11-02T13:35:35.969514Z",
     "shell.execute_reply": "2024-11-02T13:35:35.968116Z"
    },
    "papermill": {
     "duration": 16.290651,
     "end_time": "2024-11-02T13:35:35.972210",
     "exception": false,
     "start_time": "2024-11-02T13:35:19.681559",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original columns in labels_df: Index(['CASE OF CAVANI v. HUNGARY', 'v'], dtype='object')\n",
      "Shape of X: (253, 3251)\n",
      "Shape of y: (253,)\n",
      "Number of 1's: 126\n",
      "Number of -1's: 127\n",
      "label\n",
      "-1    127\n",
      " 1    126\n",
      "Name: count, dtype: int64\n",
      "Balanced Set\n",
      "Fold 1 class distribution: {-1: 102, 1: 100}\n",
      "Fold 2 class distribution: {-1: 101, 1: 101}\n",
      "Fold 3 class distribution: {-1: 101, 1: 101}\n",
      "Fold 4 class distribution: {-1: 102, 1: 101}\n",
      "Fold 5 class distribution: {-1: 102, 1: 101}\n",
      "Best Score: 0.7589803921568627\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load labels\n",
    "labels_df = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article8/cases_a8.csv\")\n",
    "\n",
    "# Load N-gram features for each case section\n",
    "ngrams_full = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article8/ngrams_a8_full.csv\")\n",
    "ngrams_procedure = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article8/ngrams_a8_procedure.csv\")\n",
    "ngrams_circumstances = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article8/ngrams_a8_circumstanes.csv\")\n",
    "ngrams_relevant_law = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article8/ngrams_a8_relevantLaw.csv\")\n",
    "ngrams_law = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article8/ngrams_a8_laws.csv\")\n",
    "\n",
    "# Combine relevant N-gram features (e.g., \"Facts\" = Circumstances + Relevant Law+ procedure + full +law)\n",
    "ngrams_facts = ngrams_circumstances.add(ngrams_relevant_law, fill_value=0)\n",
    "ngrams_facts = ngrams_facts.add(ngrams_procedure, fill_value=0)\n",
    "ngrams_facts = ngrams_facts.add(ngrams_law, fill_value=0)\n",
    "ngrams_facts = ngrams_facts.add(ngrams_full, fill_value=0)\n",
    "# Load topics and vocabulary files\n",
    "topics_df = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article8/topics8.csv\")\n",
    "ngrams_feature_names = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article8/ngrams_a8_featureNames.txt\")\n",
    "topics_vocab = pd.read_csv(\"/kaggle/input/articles-data/echr_dataset/Article8/ngrams_a8_featureNames.txt\")\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "# Create similarity matrix for N-grams\n",
    "ngram_vectors = ngrams_full.to_numpy()  # Assuming ngrams_full contains vector representations\n",
    "similarity_matrix = cosine_similarity(ngram_vectors)\n",
    "\n",
    "# Perform spectral clustering\n",
    "n_clusters = 30\n",
    "spectral_clust = SpectralClustering(n_clusters=n_clusters, affinity='precomputed', random_state=42)\n",
    "clusters = spectral_clust.fit_predict(similarity_matrix)\n",
    "\n",
    "# Add clusters as topics to topic features DataFrame\n",
    "topics_df['topic'] = clusters\n",
    "# Convert clusters (topics) into a DataFrame if it's not already\n",
    "topics_df = pd.DataFrame(clusters, columns=['topic'])\n",
    "\n",
    "# Concatenate topics to ngrams_facts along columns\n",
    "ngrams_facts = pd.concat([ngrams_facts, topics_df], axis=1)\n",
    "# Inspect the columns of labels_df\n",
    "print(\"Original columns in labels_df:\", labels_df.columns)\n",
    "\n",
    "# If there are more columns, select only the ones you need\n",
    "# Assuming the required columns are the first two:\n",
    "labels_df = labels_df.iloc[:, :2]  # Selecting the first two columns\n",
    "\n",
    "# Rename columns\n",
    "labels_df.columns = ['case_name', 'violation_status']\n",
    "\n",
    "# Map 'v' to +1 and 'nv' to -1 in the labels DataFrame\n",
    "labels_df['label'] = labels_df['violation_status'].apply(lambda x: -1 if x == 'nv' else 1)\n",
    "\n",
    "# Combine the dataframes as before\n",
    "data_df = pd.concat([ngrams_facts, labels_df['label']], axis=1).dropna()\n",
    "\n",
    "#Feature labelling for X and y\n",
    "X = data_df.drop('label', axis=1)\n",
    "y = data_df['label']\n",
    "\n",
    "# Verify shapes of X and y\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\")\n",
    "X = pd.get_dummies(X, drop_first=True) \n",
    "\n",
    "# Drop any rows with NaN values, if applicable\n",
    "X = X.dropna()\n",
    "\n",
    "\n",
    "# If y is a Pandas Series\n",
    "count = pd.Series(y).value_counts()\n",
    "print(\"Number of 1's:\", count.get(1))\n",
    "print(\"Number of -1's:\", count.get(-1))\n",
    "\n",
    "# Get counts of all unique values\n",
    "counts = y.value_counts()\n",
    "print(counts)\n",
    "print(\"Balanced Set\")\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "# Initialize SVM model\n",
    "svm_model = SVC()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Use StratifiedKFold with n_splits=5 to ensure class distribution is maintained\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Verify class distribution in each fold\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
    "    y_train_fold = y[train_idx]\n",
    "    unique_classes, counts = np.unique(y_train_fold, return_counts=True)\n",
    "    print(f\"Fold {fold_idx + 1} class distribution:\", dict(zip(unique_classes, counts)))\n",
    "\n",
    "# Initialize GridSearchCV with StratifiedKFold\n",
    "grid_search = GridSearchCV(svm_model, param_grid, cv=cv, scoring='accuracy', error_score='raise')\n",
    "\n",
    "# Fit the model with grid search\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Best model and score\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"Best Score: {grid_search.best_score_}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5995606,
     "sourceId": 9785635,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 50.059956,
   "end_time": "2024-11-02T13:35:36.701598",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-02T13:34:46.641642",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
